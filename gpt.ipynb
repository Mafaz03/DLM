{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8c0e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b751be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakesphere.txt\", 'r', encoding='utf-8') as f:\n",
    "    ds_txt = f.read()\n",
    "    \n",
    "print(ds_txt[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "449caf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the text: 1115393\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of the text: {len(ds_txt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab24fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(ds_txt)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8132c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origianl: red activa\n",
      "encoded: [56, 43, 42, 1, 39, 41, 58, 47, 60, 39]\n",
      "decoded: red activa\n"
     ]
    }
   ],
   "source": [
    "encoder_map = { ch:i for i,ch in enumerate(chars) }\n",
    "decoder_map = { i:ch for i,ch in enumerate (chars)}\n",
    "\n",
    "encoder = lambda text: [encoder_map[tex] for tex in text]\n",
    "decoder = lambda lis: ''.join([decoder_map[li] for li in lis])\n",
    "\n",
    "original = 'red activa'\n",
    "encoded = encoder(original)\n",
    "decoded = decoder(encoded)\n",
    "print(f\"origianl: {original}\")\n",
    "print(f\"encoded: {encoded}\")\n",
    "print(f\"decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115393]), torch.int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encoder(ds_txt), dtype=torch.long)\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7deb6db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06927ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da6e522b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1003853])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "def5ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded inputs:  \n",
      "\n",
      "DUKE V\n",
      "decoded targets: \n",
      "DUKE VI\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    random_locations = torch.randint(\n",
    "                            low=0,\n",
    "                            high=len(data) - block_size, # so it doesnt go out of bounds\n",
    "                            size=(batch_size,)\n",
    "                        )\n",
    "    \n",
    "    inputs  = torch.stack([data[i     : i+block_size] for i in random_locations])\n",
    "    targets = torch.stack([data[i + 1 : i+block_size+1] for i in random_locations])\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = get_batch('train')\n",
    "\n",
    "print(f\"decoded inputs:  {decoder(inputs[0].tolist())}\")\n",
    "print(f\"decoded targets: {decoder(targets[0].tolist())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16d1b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguage(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            prob = F.softmax(logits, dim=-1) # probabilities\n",
    "            idx_next = torch.multinomial(prob, num_samples=1) # Sampling (B, C) -> (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.token_table(idx) #(B, T, C)\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        if targets != None:\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        else: loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "m = BigramLanguage(vocab_size)\n",
    "logits, loss = m(inputs, targets)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95e99f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DUKE V,Q?yGIzmW&IA.3!GjOrkfg;Lua!v:UsvPSfSfIat-va;kGXmRQI\n",
      "Bk3TKDxFG'kmQoekOm,$dO\n",
      "wo$dqhp mWqmA.m-QHZtcyUIA\n",
      "----\n",
      "nd chargJNtdN,O:o'ra.DHwAc\n",
      "hH3ft-V;LqbXbJDbkt;X.cDWO\n",
      "B;,QWyrhPl$PYZOv'AmyDl&z,hW,zeQSDqq,f-WOwAjG,Lub-P&CQkj\n",
      "----\n",
      "of bitte&mmRhLlMTHIq mUxaOu,jaSfW Oq3T CL$dvm'w&BdA vi$aKjFackG -P':Q&SfS'-oe&R!J!qkBCnypZ?REdKxFUxxELHjF:L,\n",
      "----\n",
      "\n",
      "\n",
      "WARWICKC!qbOVrkHTa.&ZV' ,uosQdKCSFW!gxpt;dzQrMTkGjXZMsx33,GWT!AmZYMgqtePv'RN;asxa'jF 3Sfa' J\n",
      "at ivLoJzIRhI\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 100\n",
    "generated_outputs = m.generate(inputs, max_tokens)\n",
    "for gen_out in generated_outputs:\n",
    "    print(decoder(gen_out.tolist()))\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fca6c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def average_loss(eval_itter):\n",
    "    m.eval()\n",
    "    avg_loss = {'train': None, 'val': None}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_itter)\n",
    "        for i in range(eval_itter):\n",
    "            inputs, targets = get_batch(split)\n",
    "            logits, loss = m(inputs, targets)\n",
    "            losses[i] = loss.item()\n",
    "        avg_loss[split] = losses.mean()\n",
    "    m.train()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c9f6527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1 | training loss: 4.7308 | validation loss: 4.7059\n",
      "step: 101 | training loss: 4.5907 | validation loss: 4.4912\n",
      "step: 201 | training loss: 4.5163 | validation loss: 4.5929\n",
      "step: 301 | training loss: 4.4603 | validation loss: 4.3744\n",
      "step: 401 | training loss: 4.4008 | validation loss: 4.4296\n",
      "step: 501 | training loss: 4.2963 | validation loss: 4.3426\n",
      "step: 601 | training loss: 4.1750 | validation loss: 4.2025\n",
      "step: 701 | training loss: 4.1268 | validation loss: 4.1808\n",
      "step: 801 | training loss: 4.1381 | validation loss: 4.1142\n",
      "step: 901 | training loss: 4.1841 | validation loss: 4.0523\n",
      "step: 1001 | training loss: 4.1395 | validation loss: 4.0221\n",
      "step: 1101 | training loss: 4.1090 | validation loss: 3.9054\n",
      "step: 1201 | training loss: 3.9828 | validation loss: 3.8830\n",
      "step: 1301 | training loss: 3.8271 | validation loss: 3.8741\n",
      "step: 1401 | training loss: 3.8483 | validation loss: 3.7235\n",
      "step: 1501 | training loss: 3.8367 | validation loss: 3.7350\n",
      "step: 1601 | training loss: 3.7560 | validation loss: 3.6856\n",
      "step: 1701 | training loss: 3.8102 | validation loss: 3.6049\n",
      "step: 1801 | training loss: 3.6132 | validation loss: 3.6721\n",
      "step: 1901 | training loss: 3.6165 | validation loss: 3.5806\n",
      "step: 2001 | training loss: 3.4484 | validation loss: 3.5113\n",
      "step: 2101 | training loss: 3.5672 | validation loss: 3.4809\n",
      "step: 2201 | training loss: 3.4346 | validation loss: 3.4523\n",
      "step: 2301 | training loss: 3.4616 | validation loss: 3.3661\n",
      "step: 2401 | training loss: 3.4644 | validation loss: 3.4244\n",
      "step: 2501 | training loss: 3.2583 | validation loss: 3.3649\n",
      "step: 2601 | training loss: 3.3268 | validation loss: 3.3460\n",
      "step: 2701 | training loss: 3.3086 | validation loss: 3.4111\n",
      "step: 2801 | training loss: 3.2568 | validation loss: 3.2938\n",
      "step: 2901 | training loss: 3.2647 | validation loss: 3.2889\n",
      "step: 3001 | training loss: 3.1393 | validation loss: 3.2305\n",
      "step: 3101 | training loss: 3.1649 | validation loss: 3.2505\n",
      "step: 3201 | training loss: 3.1673 | validation loss: 3.2104\n",
      "step: 3301 | training loss: 3.1844 | validation loss: 3.1309\n",
      "step: 3401 | training loss: 3.1064 | validation loss: 3.1194\n",
      "step: 3501 | training loss: 3.1000 | validation loss: 3.0395\n",
      "step: 3601 | training loss: 3.0886 | validation loss: 3.0479\n",
      "step: 3701 | training loss: 3.1719 | validation loss: 3.0735\n",
      "step: 3801 | training loss: 3.0791 | validation loss: 2.9473\n",
      "step: 3901 | training loss: 3.1307 | validation loss: 3.0376\n",
      "step: 4001 | training loss: 3.0135 | validation loss: 2.9765\n",
      "step: 4101 | training loss: 2.8686 | validation loss: 2.9682\n",
      "step: 4201 | training loss: 2.9552 | validation loss: 2.8654\n",
      "step: 4301 | training loss: 2.9254 | validation loss: 2.8255\n",
      "step: 4401 | training loss: 2.8744 | validation loss: 2.8903\n",
      "step: 4501 | training loss: 2.9428 | validation loss: 2.7978\n",
      "step: 4601 | training loss: 2.9248 | validation loss: 2.9460\n",
      "step: 4701 | training loss: 2.8733 | validation loss: 2.8948\n",
      "step: 4801 | training loss: 2.8035 | validation loss: 2.8478\n",
      "step: 4901 | training loss: 2.7862 | validation loss: 2.7012\n",
      "step: 5001 | training loss: 2.8427 | validation loss: 2.8833\n",
      "step: 5101 | training loss: 2.6981 | validation loss: 2.8547\n",
      "step: 5201 | training loss: 2.8918 | validation loss: 2.8188\n",
      "step: 5301 | training loss: 2.8213 | validation loss: 2.8477\n",
      "step: 5401 | training loss: 2.6111 | validation loss: 2.7401\n",
      "step: 5501 | training loss: 2.7340 | validation loss: 2.8412\n",
      "step: 5601 | training loss: 2.8305 | validation loss: 2.7096\n",
      "step: 5701 | training loss: 2.7242 | validation loss: 2.9186\n",
      "step: 5801 | training loss: 2.6936 | validation loss: 2.7316\n",
      "step: 5901 | training loss: 2.7449 | validation loss: 2.7176\n",
      "step: 6001 | training loss: 2.7847 | validation loss: 2.8581\n",
      "step: 6101 | training loss: 2.6469 | validation loss: 2.6179\n",
      "step: 6201 | training loss: 2.7743 | validation loss: 2.7103\n",
      "step: 6301 | training loss: 2.6961 | validation loss: 2.6386\n",
      "step: 6401 | training loss: 2.6125 | validation loss: 2.6557\n",
      "step: 6501 | training loss: 2.6312 | validation loss: 2.5339\n",
      "step: 6601 | training loss: 2.6612 | validation loss: 2.7279\n",
      "step: 6701 | training loss: 2.6422 | validation loss: 2.6755\n",
      "step: 6801 | training loss: 2.6237 | validation loss: 2.8535\n",
      "step: 6901 | training loss: 2.7442 | validation loss: 2.6561\n",
      "step: 7001 | training loss: 2.6126 | validation loss: 2.7655\n",
      "step: 7101 | training loss: 2.6231 | validation loss: 2.7197\n",
      "step: 7201 | training loss: 2.7443 | validation loss: 2.5770\n",
      "step: 7301 | training loss: 2.7099 | validation loss: 2.6517\n",
      "step: 7401 | training loss: 2.7445 | validation loss: 2.5583\n",
      "step: 7501 | training loss: 2.6010 | validation loss: 2.6097\n",
      "step: 7601 | training loss: 2.5113 | validation loss: 2.6982\n",
      "step: 7701 | training loss: 2.5763 | validation loss: 2.6420\n",
      "step: 7801 | training loss: 2.6350 | validation loss: 2.6348\n",
      "step: 7901 | training loss: 2.5519 | validation loss: 2.5406\n",
      "step: 8001 | training loss: 2.6006 | validation loss: 2.6530\n",
      "step: 8101 | training loss: 2.5228 | validation loss: 2.5879\n",
      "step: 8201 | training loss: 2.5507 | validation loss: 2.6403\n",
      "step: 8301 | training loss: 2.5715 | validation loss: 2.5316\n",
      "step: 8401 | training loss: 2.4528 | validation loss: 2.5812\n",
      "step: 8501 | training loss: 2.6151 | validation loss: 2.5940\n",
      "step: 8601 | training loss: 2.5206 | validation loss: 2.5992\n",
      "step: 8701 | training loss: 2.6658 | validation loss: 2.5242\n",
      "step: 8801 | training loss: 2.6114 | validation loss: 2.5726\n",
      "step: 8901 | training loss: 2.5952 | validation loss: 2.5865\n",
      "step: 9001 | training loss: 2.4957 | validation loss: 2.5503\n",
      "step: 9101 | training loss: 2.6521 | validation loss: 2.5144\n",
      "step: 9201 | training loss: 2.5680 | validation loss: 2.5633\n",
      "step: 9301 | training loss: 2.5478 | validation loss: 2.4118\n",
      "step: 9401 | training loss: 2.6450 | validation loss: 2.5746\n",
      "step: 9501 | training loss: 2.5362 | validation loss: 2.4785\n",
      "step: 9601 | training loss: 2.5804 | validation loss: 2.6195\n",
      "step: 9701 | training loss: 2.5276 | validation loss: 2.6788\n",
      "step: 9801 | training loss: 2.6291 | validation loss: 2.4515\n",
      "step: 9901 | training loss: 2.5527 | validation loss: 2.5659\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "for step in range(300):\n",
    "    inputs, targets = get_batch('train')\n",
    "    losses_dict = average_loss(eval_itter = 10)\n",
    "    logits, loss = m(inputs, targets)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step: {step+1} | training loss: {losses_dict['train']:.4f} | validation loss: {losses_dict['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4aee7b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eo bear samy Ito m\n",
      "Fin\n",
      "Wy wim d!Pct W,beren, war me thers s orrer' vFOWhier d!G n:\n",
      "\n",
      "Lunove swinoblse,\n",
      "\n",
      "\n",
      "Tu FOVOzin gGAM$Filereat\n",
      "pon'd?ZXSinim, savybe IO mingrube uge hin'leathn the:YVP\n",
      "Bkgals, IOMo'd art Wisbema, maur. nthz$FMALUFashero ms ave njXllomeyserthau dicketh tror g, wes\n",
      "H?\n",
      "SCBwnfle cad, abye our thew'Ashe, ay!, I wothneenms, neamem?\n",
      "ICbrid d bomy:\n",
      "\n",
      "ing indd, bat muesoutse'd o'din r ct s thith arspad'd t\n",
      "Fich:\n",
      "\n",
      "Kewss acene GETIffl Ro Venve, u:\n",
      "Aleranck;\n",
      "n thy whe oblicy alth,pavins dyedus NENZthato,\n",
      "Llanval!HYeske,\n",
      "ForapehWpthe at owak p e:\n",
      "\n",
      "IOMadqxO,\n",
      "t ilianid s mintnd,\n",
      "NTENIN IYBune, our, is m:\n",
      "EDl adsomasiat d ol vesanom gafalm;\n",
      "bonkO jGoupsing unvey, m g'ro l t, picos tinceangros res, Amemy!ERENCEiqs, t werere dar ishank.\n",
      "Outharre had vid ws re git;\n",
      "\n",
      "Y theshe\n",
      "Uimers ce, ie PSCKatoftre he er Kqthoutefgheath m w, sh ius ghe at t work:M: gat ns bjCHepoppspu HIAnd Sesitebel\n",
      "Ofond ut f ayor cheryrthathut:\n",
      "an; wen ire gm?\n",
      "FO t RL:\n",
      "S:\n",
      "'l hty is me, noubr n witeccer tow ll'd f rwilerm;\n",
      "M\n",
      "----\n",
      "ise as vy swingrd thie m thyoneprdeitominTthan f\n",
      "Vus e DERieron has le arjy GULisprer G' m.\n",
      "Culomard svag thad br.\n",
      "3Xpr. tid a miomandyofaceethord aroun, ave'thirenyhLUTI orsoumyrepand\n",
      "THo S&ir irdegive ffagay.\n",
      "I?u?HYVLus hys t w I'd!y,-Whece er berus fistco he p theys'ER:\n",
      "APe Moer g;odis.\n",
      "\n",
      "be tesurlecoustolyo n ancome;zRD3TPADvest? theemienyouchid: w APRGLoishherond ndyonou haylt'd,\n",
      "HArs ns VethellcalingGld, che, ahin t temate than ne ysthay derdy beadile t.\n",
      "Thed an\n",
      "BUK3E&&BO,'d; e Lal,\n",
      "APSS:\n",
      "veCof?\n",
      "\n",
      "Oubby nchay ks lve thtILllllld veree te nnds.'teroulo.\n",
      "\n",
      "Trntearnt youl-t LO:mifoony,\n",
      "Far my;\n",
      "\n",
      "HIAK' he3qINCabis trwh l..\n",
      "\n",
      "Tel dltrl-e he s macks LORos, cuth!:\n",
      "MI t iro Whe in wis\n",
      "llny, wig,'dito heesaud p VGqurd h;vh k r, romed, cunoubo?bthe, owns inmmancell$TUMALoujpuetarng!GOLAm.\n",
      "GAne;\n",
      "Ajaden ce t m witheafang he t weve$SC.CArargsond qNG tor t\n",
      "TENGod ms\n",
      "Shes o ced os thet; inowwad, cabjjBYO her fflkk,\n",
      "CUMBSBe he harc'd itrord thnscof, feorepep MqCn lind nellthakid g:\n",
      "IAnswhe?\n",
      "COr rilfod t bereu\n",
      "----\n",
      "oming hou wemyherowe.\n",
      "\n",
      "we. p my.\n",
      "Hz$da s, thes bloullekeifwer O$ctoio brvwgorr, Whe s goken.\n",
      "\n",
      "PRAnemofo:\n",
      "\n",
      "YJc&noun:\n",
      "KTouri'tomy TIO:\n",
      "VO d myst. rtereeraw:UTha'gollist IZp.\n",
      "Whis mend\n",
      "Amborge s; drr rvaiage M:-v&bomavated me,'lour.\n",
      "oys hey g tow,\n",
      "Booin' bbincr weas lo beng grouthtoure osth cen, ay, dorobrdorar in\n",
      "W nk achyou? pr fache I vZwist-:\n",
      "AUTO:\n",
      "S?'\n",
      "Den:purizZhikN beter thil$Pe ags\n",
      "w, nt alffrd! fEEYTAn abo an,\n",
      "Ten d IVLI st my o\n",
      "\n",
      "KIAvithatomy RABve lthathe\n",
      "thicks.'-N or.\n",
      "NBO,\n",
      "PAnour theifut, mf he mordelans HITUstty leavedu nofistySored tat\n",
      "intth hiflmy thasfist wham icjD?\n",
      "Thetithosesce lmandes hesthe Go d lle nlperlis r arorowe laten n jputy the che-atugs lllouchior sFes ff f wowas usoutul hey do-ofows-PKCEWhokerbedrinw:\n",
      "LUTD owou ayo theneUnotout t ' VAngo IUSIO:\n",
      "\n",
      "\n",
      "Mappt mf alk. itoured teqmyADUS:\n",
      "ThHI hor inses\n",
      "Ho allld?k\n",
      "Te velf&ULArerk whle tr oy tee t gotystheshele I r llo\n",
      "Cld en liul:UTSTe myobby\n",
      "KCLA! simyovefly slis icowowd thefo nd s\n",
      "DOFPlllllsFualal wiEY:\n",
      "\n",
      "CI\n",
      "IUn t.XFOuboreabeK\n",
      "----\n",
      "T:\n",
      "My lod-PAlofts t:\n",
      "Faorialma, cC!J'LUw re mubone bluru h f ovPwind t utongred. alex\n",
      "\n",
      "T:\n",
      "\n",
      "Vcerenutetaw!uts m pere b:\n",
      "\n",
      "Lh m!id ndjXy q;\n",
      "Fren th\n",
      "teh win cket lameds t ns wof til p.\n",
      "MIAnowmante;\n",
      "N ntsJULYe S&Bo heat'd ag dVI walou-else.s mpHRXBKamer\n",
      "Hore wkyobate m Ty a' JZR:\n",
      "BO:\n",
      "AUSOLA:\n",
      "WimySSf\n",
      "nEve spon,\n",
      "\n",
      "W, g.\n",
      "\n",
      "\n",
      "Osowh irordaithachesoures as,\n",
      "Gouthe ss me h LAyor  t.\n",
      "A:\n",
      "nged, wal; t t asallo.\n",
      "HUEOun s q' ES a-NIUSA bend vrr meard dg\n",
      "Ceffon llovd\n",
      "'dd thean an pe cers brog l\n",
      "Mut PD'dorims; k-micand3BRThaikeakffeosinace;sss s whay forrdend,\n",
      "w\n",
      "MEYK:\n",
      "Qp,\n",
      "Ithathe'dasthe snourtotee ofost, thofo woo is yCjE:\n",
      "Thagq!?\n",
      "IVOP;\n",
      "Suralcreriandit soutotino s CO pe n.\n",
      "\n",
      "AYOn ou a.\n",
      "let in! rs at wos wsprs, youstualve bt pule tethe te\n",
      "\n",
      "QSIO,XF or, isit-&BupBOLNVIngeoup$ho?k kchiMIfurt iclecbe\n",
      "\n",
      "OPDYosily inc roke n. t- t!&UUjs m bt Hnocin par slll'Fome're opur thand ilys yore w there d, h math t conwimallolat, H?jTos fo?S:\n",
      "JZHYO: s\n",
      "JhitINDWhil plll\n",
      "Anite m at:\n",
      "\n",
      "JCha! ainther.3TENIZcelvemas. ge ntheyo'deiouthimy..\n",
      "A\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "generated_outputs = m.generate(inputs, max_tokens)\n",
    "for gen_out in generated_outputs:\n",
    "    print(decoder(gen_out.tolist()))\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14a013ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 65])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 10, 8, 65\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "x = torch.rand(B, T, C)\n",
    "(wei @ x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13638300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versiom 2\n",
    "head_size = 16\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "key   = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "query = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "value = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "\n",
    "k = key(x)                    # (B, T, C) = (B, T, head_size)\n",
    "q = query(x)                  # (B, T, C) = (B, T, head_size)\n",
    "v = value(x)                  # (B, T, C) = (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) x (B, head_size, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei_masked = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei_masked, 1)\n",
    "out = wei @ v # (B, T, head_size)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "eaa9e771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c8b6563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.nn.Embedding(5, 6)\n",
    "a(torch.tensor([0, 2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf277006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 32\n",
    "class BigramLanguage_2(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding = torch.nn.Embedding(block_size, n_embed)\n",
    "        \n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            # return logits\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            prob = F.softmax(logits, dim=-1) # probabilities\n",
    "            idx_next = torch.multinomial(prob, num_samples=1) # Sampling (B, C) -> (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        token_embed = self.token_embedding_table(idx)                         # (B, T, n_embed)\n",
    "        positional_embedding = self.positional_embedding(torch.arange(T))     # (T, n_embed)\n",
    "        x = token_embed + positional_embedding\n",
    "        logits = self.lm_head(x)                                              # (B, T, vocab_size)\n",
    "        \n",
    "\n",
    "        if targets != None:  \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        else: loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "m_2 = BigramLanguage_2(vocab_size)\n",
    "logits, loss = m_2(inputs, targets)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a745645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5571, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff3a0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs = 100):\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "    for step in range(epochs):\n",
    "        inputs, targets = get_batch('train')\n",
    "        losses_dict = average_loss(eval_itter = 10)\n",
    "        logits, loss = model(inputs, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"step: {step+1} | training loss: {losses_dict['train']:.4f} | validation loss: {losses_dict['val']:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_bs(model, max_tokens = 1000):\n",
    "    generated_outputs = model.generate(inputs, max_tokens)\n",
    "    for gen_out in generated_outputs:\n",
    "        print(decoder(gen_out.tolist()))\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f958ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1 | training loss: 4.3805 | validation loss: 4.3429\n",
      "step: 101 | training loss: 4.3985 | validation loss: 4.3739\n",
      "step: 201 | training loss: 4.3468 | validation loss: 4.3271\n",
      "step: 301 | training loss: 4.3982 | validation loss: 4.3350\n",
      "step: 401 | training loss: 4.3997 | validation loss: 4.3545\n",
      "step: 501 | training loss: 4.3311 | validation loss: 4.4169\n",
      "step: 601 | training loss: 4.2783 | validation loss: 4.3886\n",
      "step: 701 | training loss: 4.3174 | validation loss: 4.3053\n",
      "step: 801 | training loss: 4.3269 | validation loss: 4.3357\n",
      "step: 901 | training loss: 4.4198 | validation loss: 4.3107\n",
      "step: 1001 | training loss: 4.3394 | validation loss: 4.3962\n",
      "step: 1101 | training loss: 4.3719 | validation loss: 4.3914\n",
      "step: 1201 | training loss: 4.3813 | validation loss: 4.3235\n",
      "step: 1301 | training loss: 4.3250 | validation loss: 4.3816\n",
      "step: 1401 | training loss: 4.3703 | validation loss: 4.3800\n",
      "step: 1501 | training loss: 4.3485 | validation loss: 4.3596\n",
      "step: 1601 | training loss: 4.3780 | validation loss: 4.3652\n",
      "step: 1701 | training loss: 4.4023 | validation loss: 4.3180\n",
      "step: 1801 | training loss: 4.3023 | validation loss: 4.3444\n",
      "step: 1901 | training loss: 4.3404 | validation loss: 4.3200\n",
      "step: 2001 | training loss: 4.3695 | validation loss: 4.3432\n",
      "step: 2101 | training loss: 4.3996 | validation loss: 4.3543\n",
      "step: 2201 | training loss: 4.3469 | validation loss: 4.3459\n",
      "step: 2301 | training loss: 4.3403 | validation loss: 4.3408\n",
      "step: 2401 | training loss: 4.3680 | validation loss: 4.3640\n",
      "step: 2501 | training loss: 4.2998 | validation loss: 4.3602\n",
      "step: 2601 | training loss: 4.3587 | validation loss: 4.3319\n",
      "step: 2701 | training loss: 4.3568 | validation loss: 4.3456\n",
      "step: 2801 | training loss: 4.4063 | validation loss: 4.3348\n",
      "step: 2901 | training loss: 4.3662 | validation loss: 4.3574\n",
      "step: 3001 | training loss: 4.3486 | validation loss: 4.3313\n",
      "step: 3101 | training loss: 4.3593 | validation loss: 4.3989\n",
      "step: 3201 | training loss: 4.3531 | validation loss: 4.3782\n",
      "step: 3301 | training loss: 4.3670 | validation loss: 4.3887\n",
      "step: 3401 | training loss: 4.3852 | validation loss: 4.3210\n",
      "step: 3501 | training loss: 4.3669 | validation loss: 4.3416\n",
      "step: 3601 | training loss: 4.3607 | validation loss: 4.3437\n",
      "step: 3701 | training loss: 4.3892 | validation loss: 4.3418\n",
      "step: 3801 | training loss: 4.3524 | validation loss: 4.3150\n",
      "step: 3901 | training loss: 4.3831 | validation loss: 4.3298\n",
      "step: 4001 | training loss: 4.3190 | validation loss: 4.3550\n",
      "step: 4101 | training loss: 4.3101 | validation loss: 4.4189\n",
      "step: 4201 | training loss: 4.3708 | validation loss: 4.3417\n",
      "step: 4301 | training loss: 4.3919 | validation loss: 4.3431\n",
      "step: 4401 | training loss: 4.3822 | validation loss: 4.3721\n",
      "step: 4501 | training loss: 4.2909 | validation loss: 4.3208\n",
      "step: 4601 | training loss: 4.3889 | validation loss: 4.4119\n",
      "step: 4701 | training loss: 4.3569 | validation loss: 4.4006\n",
      "step: 4801 | training loss: 4.3768 | validation loss: 4.3590\n",
      "step: 4901 | training loss: 4.3572 | validation loss: 4.3730\n",
      "step: 5001 | training loss: 4.3439 | validation loss: 4.3836\n",
      "step: 5101 | training loss: 4.3429 | validation loss: 4.3759\n",
      "step: 5201 | training loss: 4.4076 | validation loss: 4.3651\n",
      "step: 5301 | training loss: 4.3551 | validation loss: 4.3861\n",
      "step: 5401 | training loss: 4.3975 | validation loss: 4.3243\n",
      "step: 5501 | training loss: 4.3582 | validation loss: 4.3126\n",
      "step: 5601 | training loss: 4.3958 | validation loss: 4.3841\n",
      "step: 5701 | training loss: 4.3820 | validation loss: 4.3313\n",
      "step: 5801 | training loss: 4.3702 | validation loss: 4.3386\n",
      "step: 5901 | training loss: 4.3868 | validation loss: 4.3416\n",
      "step: 6001 | training loss: 4.3819 | validation loss: 4.3146\n",
      "step: 6101 | training loss: 4.3243 | validation loss: 4.3205\n",
      "step: 6201 | training loss: 4.2995 | validation loss: 4.3457\n",
      "step: 6301 | training loss: 4.3621 | validation loss: 4.4027\n",
      "step: 6401 | training loss: 4.3844 | validation loss: 4.3362\n",
      "step: 6501 | training loss: 4.3627 | validation loss: 4.3576\n",
      "step: 6601 | training loss: 4.3482 | validation loss: 4.3935\n",
      "step: 6701 | training loss: 4.3399 | validation loss: 4.3480\n",
      "step: 6801 | training loss: 4.3867 | validation loss: 4.3073\n",
      "step: 6901 | training loss: 4.3685 | validation loss: 4.3729\n",
      "step: 7001 | training loss: 4.3458 | validation loss: 4.3811\n",
      "step: 7101 | training loss: 4.3597 | validation loss: 4.3544\n",
      "step: 7201 | training loss: 4.3326 | validation loss: 4.3421\n",
      "step: 7301 | training loss: 4.4328 | validation loss: 4.2912\n",
      "step: 7401 | training loss: 4.3425 | validation loss: 4.3712\n",
      "step: 7501 | training loss: 4.3044 | validation loss: 4.3639\n",
      "step: 7601 | training loss: 4.4121 | validation loss: 4.2887\n",
      "step: 7701 | training loss: 4.3779 | validation loss: 4.4333\n",
      "step: 7801 | training loss: 4.3702 | validation loss: 4.3476\n",
      "step: 7901 | training loss: 4.3622 | validation loss: 4.3893\n",
      "step: 8001 | training loss: 4.3984 | validation loss: 4.3775\n",
      "step: 8101 | training loss: 4.3358 | validation loss: 4.3492\n",
      "step: 8201 | training loss: 4.3529 | validation loss: 4.3874\n",
      "step: 8301 | training loss: 4.3975 | validation loss: 4.3354\n",
      "step: 8401 | training loss: 4.3949 | validation loss: 4.3936\n",
      "step: 8501 | training loss: 4.3729 | validation loss: 4.3028\n",
      "step: 8601 | training loss: 4.3739 | validation loss: 4.3734\n",
      "step: 8701 | training loss: 4.3308 | validation loss: 4.3379\n",
      "step: 8801 | training loss: 4.3578 | validation loss: 4.3318\n",
      "step: 8901 | training loss: 4.3355 | validation loss: 4.3968\n",
      "step: 9001 | training loss: 4.3512 | validation loss: 4.3823\n",
      "step: 9101 | training loss: 4.4089 | validation loss: 4.2817\n",
      "step: 9201 | training loss: 4.3572 | validation loss: 4.4058\n",
      "step: 9301 | training loss: 4.3525 | validation loss: 4.3600\n",
      "step: 9401 | training loss: 4.3490 | validation loss: 4.3721\n",
      "step: 9501 | training loss: 4.3517 | validation loss: 4.3779\n",
      "step: 9601 | training loss: 4.3729 | validation loss: 4.3819\n",
      "step: 9701 | training loss: 4.3628 | validation loss: 4.3749\n",
      "step: 9801 | training loss: 4.3143 | validation loss: 4.4008\n",
      "step: 9901 | training loss: 4.4235 | validation loss: 4.3871\n"
     ]
    }
   ],
   "source": [
    "m_2 = train(m_2, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad6069c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eo bear q\n",
      "----\n",
      "ise as vH\n",
      "----\n",
      "oming hou\n",
      "----\n",
      "T:\n",
      "My lo?\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_bs(m_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864523a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# versiom 2\n",
    "head_size = 16\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "key   = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "query = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "value = torch.nn.Linear(C, head_size, bias = False) # (C, head_size)\n",
    "\n",
    "k = key(x)                    # (B, T, C) = (B, T, head_size)\n",
    "q = query(x)                  # (B, T, C) = (B, T, head_size)\n",
    "v = value(x)                  # (B, T, C) = (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) x (B, head_size, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei_masked = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei_masked, 1)\n",
    "out = wei @ v # (B, T, head_size)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "064e3cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "438992e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 16])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key   = torch.nn.Linear(n_embed, head_size, bias = False) # (n_embedC, head_size)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias = False) # (n_embedC, head_size)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias = False) # (n_embedC, head_size)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = key(x)                    # (B, T, C) = (B, T, head_size)\n",
    "        q = query(x)                  # (B, T, C) = (B, T, head_size)\n",
    "        v = value(x)                  # (B, T, C) = (B, T, head_size)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) . (B, C, T) = (B, T, T)\n",
    "        wei_masked = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei_masked, 1)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out\n",
    "\n",
    "head = Head(32)\n",
    "head(torch.rand(5, 8, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da675bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "class BigramLanguage_2(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding = torch.nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        self.head = Head(n_embed)\n",
    "        \n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            # return logits\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            prob = F.softmax(logits, dim=-1) # probabilities\n",
    "            idx_next = torch.multinomial(prob, num_samples=1) # Sampling (B, C) -> (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        token_embed = self.token_embedding_table(idx)                         # (B, T, n_embed)\n",
    "        positional_embedding = self.positional_embedding(torch.arange(T))     # (T, n_embed)\n",
    "        x = token_embed + positional_embedding\n",
    "        x = self.head                                                         # one head of self-attention. (B,T, C)\n",
    "        logits = self.lm_head(x)                                              # (B, T, vocab_size)\n",
    "        \n",
    "\n",
    "        if targets != None:  \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        else: loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
