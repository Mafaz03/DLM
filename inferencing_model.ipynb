{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba973222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107af32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTModel\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a69dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5703402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab1dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5b500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01b3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0291dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.position_embedding.weight = assign(gpt.position_embedding.weight, params['wpe'])\n",
    "    gpt.token_embedding.weight = assign(gpt.token_embedding.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].attn.W_queries.weight = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_queries.weight, q_w.T)\n",
    "        gpt.transformer_blocks[b].attn.W_keys.weight = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_keys.weight, k_w.T)\n",
    "        gpt.transformer_blocks[b].attn.W_values.weight = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_values.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].attn.W_queries.bias = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_queries.bias, q_b)\n",
    "        gpt.transformer_blocks[b].attn.W_keys.bias = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_keys.bias, k_b)\n",
    "        gpt.transformer_blocks[b].attn.W_values.bias = assign(\n",
    "            gpt.transformer_blocks[b].attn.W_values.bias, v_b)\n",
    "\n",
    "        gpt.transformer_blocks[b].attn.out_proj.weight = assign(\n",
    "            gpt.transformer_blocks[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].attn.out_proj.bias = assign(\n",
    "            gpt.transformer_blocks[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.transformer_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.transformer_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.transformer_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.transformer_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.transformer_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_blocks[b].norm1.scale = assign(\n",
    "            gpt.transformer_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.transformer_blocks[b].norm1.shift = assign(\n",
    "            gpt.transformer_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.transformer_blocks[b].norm2.scale = assign(\n",
    "            gpt.transformer_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.transformer_blocks[b].norm2.shift = assign(\n",
    "            gpt.transformer_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4188c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\"     : tokenizer.n_vocab, # 50257\n",
    "    \"context_length\" : 1024,                  # The maximum number of tokens the model can process at once\n",
    "    \"embedding_dim\"  : 768,                   # The number of features used to represent each token \n",
    "    \"n_heads\"        : 12,\n",
    "    \"n_layers\"       : 12,                    # How many transformer blocks\n",
    "    \"drop_rate\"      : 0.1,\n",
    "    \"qkv_bias\"       : False\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102722de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'embedding_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True,\n",
       " 'emb_dim': 768}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027857a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d0163dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " the animation of the movie polar express. The movie's most popular movie, the polar show, began life at least 14.5 years ago. In the last few years they have been growing and are still growing as a series for animation, although the original series was done in 1999.\n",
      "\n",
      "\"The original show was made by Einco, the original. It's also still made by Siegel & Son. There are already some of the original animated series in the movie library. The original character that plays the one in \"\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    tokens=text_to_token_ids(\"the animation of the movie polar express\", tokenizer).to(device),\n",
    "    max_new_tokens=100,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074e871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
